{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00927,
     "end_time": "2020-12-31T16:38:31.835305",
     "exception": false,
     "start_time": "2020-12-31T16:38:31.826035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### adapted from https://www.kaggle.com/aimind/bottleneck-encoder-mlp-keras-tuner-8601c5\n",
    "\n",
    "\n",
    "### *** todo fix train/submission switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network for Jane Street Market Prediction on Kaggle\n",
    "# https://www.kaggle.com/c/jane-street-market-prediction\n",
    "# https://www.kaggle.com/wrinkledtime\n",
    "# https://github.com/timestocome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Jane Street competition has blinded data and the goal is to predict stock market winners 6 months from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-31T16:38:31.861984Z",
     "iopub.status.busy": "2020-12-31T16:38:31.861413Z",
     "iopub.status.idle": "2020-12-31T16:38:36.673722Z",
     "shell.execute_reply": "2020-12-31T16:38:36.672418Z"
    },
    "papermill": {
     "duration": 4.830133,
     "end_time": "2020-12-31T16:38:36.673837",
     "exception": false,
     "start_time": "2020-12-31T16:38:31.843704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, KFold, TimeSeriesSplit\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:38:36.696161Z",
     "iopub.status.busy": "2020-12-31T16:38:36.695625Z",
     "iopub.status.idle": "2020-12-31T16:41:08.350307Z",
     "shell.execute_reply": "2020-12-31T16:41:08.348893Z"
    },
    "papermill": {
     "duration": 151.66788,
     "end_time": "2020-12-31T16:41:08.350503",
     "exception": false,
     "start_time": "2020-12-31T16:38:36.682623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/jane-street-market-prediction/example_sample_submission.csv\n",
      "/kaggle/input/jane-street-market-prediction/features.csv\n",
      "/kaggle/input/jane-street-market-prediction/example_test.csv\n",
      "/kaggle/input/jane-street-market-prediction/train.csv\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/__init__.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "        \n",
    "train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:41:08.382918Z",
     "iopub.status.busy": "2020-12-31T16:41:08.382040Z",
     "iopub.status.idle": "2020-12-31T16:41:15.072709Z",
     "shell.execute_reply": "2020-12-31T16:41:15.072187Z"
    },
    "papermill": {
     "duration": 6.709796,
     "end_time": "2020-12-31T16:41:15.072827",
     "exception": false,
     "start_time": "2020-12-31T16:41:08.363031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAINING = True\n",
    "FOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "# drop rows that might skew the data\n",
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "\n",
    "# reduce memory footprint\n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "\n",
    "# drop rows with a weight of 0 ( weight tells the return on investment )\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "\n",
    "\n",
    "# only set action to buy if all returns > 0\n",
    "train['action'] =  (  (train['resp_1'] > 0.00001 ) & (train['resp_2'] > 0.00001 ) & (train['resp_3'] > 0.00001 ) & (train['resp_4'] > 0.00001 ) &  (train['resp'] > 0.00001 )   ).astype('int')\n",
    "\n",
    "\n",
    "# collect feature columns, should probably drop feature_0\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "\n",
    "# convert inputs to numpy\n",
    "X = train[features].values\n",
    "\n",
    "\n",
    "# collect all 5 return values if > 0\n",
    "y = np.stack([(train[c] > 0.000001).astype('int') for c in resp_cols]).T # Multitarget\n",
    "\n",
    "\n",
    "# Calculate means to use to fill in nan\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:41:15.101829Z",
     "iopub.status.busy": "2020-12-31T16:41:15.100044Z",
     "iopub.status.idle": "2020-12-31T16:41:15.102434Z",
     "shell.execute_reply": "2020-12-31T16:41:15.102859Z"
    },
    "papermill": {
     "duration": 0.020692,
     "end_time": "2020-12-31T16:41:15.102957",
     "exception": false,
     "start_time": "2020-12-31T16:41:15.082265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deep Bottleneck Classifiers in Supervised Dimension Reduction\n",
    "# https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICANN-2010/papers/6354/63540001.pdf\n",
    "\n",
    "def create_autoencoder(input_dim, output_dim, noise=0.05):\n",
    "\n",
    "    i = Input(input_dim)\n",
    "    \n",
    "    # normalize input, add Gaussian noise, relu make network asymmetric, dropout reduces overfitting, restore input\n",
    "    encoded = BatchNormalization()(i)\n",
    "    encoded = GaussianNoise(noise)(encoded)\n",
    "    encoded = Dense(640,activation='relu')(encoded)\n",
    "    decoded = Dropout(0.2)(encoded)\n",
    "    decoded = Dense(input_dim, name='decoded')(decoded)\n",
    "    \n",
    "    # take decoded input, make network asymmetric (relu), normalize it, dropout, fit to targets\n",
    "    x = Dense(320,activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(output_dim, activation='sigmoid', name='label_output')(x)\n",
    "    \n",
    "    encoder = Model(inputs=i,outputs=encoded)\n",
    "    autoencoder = Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:41:15.127006Z",
     "iopub.status.busy": "2020-12-31T16:41:15.126484Z",
     "iopub.status.idle": "2020-12-31T16:43:15.887842Z",
     "shell.execute_reply": "2020-12-31T16:43:15.886737Z"
    },
    "papermill": {
     "duration": 120.776198,
     "end_time": "2020-12-31T16:43:15.887965",
     "exception": false,
     "start_time": "2020-12-31T16:41:15.111767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1002\n",
      "87/87 [==============================] - 3s 29ms/step - loss: 2.1730 - decoded_loss: 1.4354 - label_output_loss: 0.7376 - val_loss: 1.1008 - val_decoded_loss: 0.4069 - val_label_output_loss: 0.6940\n",
      "Epoch 2/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 1.1442 - decoded_loss: 0.4415 - label_output_loss: 0.7027 - val_loss: 0.8376 - val_decoded_loss: 0.1480 - val_label_output_loss: 0.6896\n",
      "Epoch 3/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 1.0219 - decoded_loss: 0.3279 - label_output_loss: 0.6940 - val_loss: 0.7861 - val_decoded_loss: 0.0974 - val_label_output_loss: 0.6887\n",
      "Epoch 4/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9805 - decoded_loss: 0.2895 - label_output_loss: 0.6910 - val_loss: 0.7700 - val_decoded_loss: 0.0812 - val_label_output_loss: 0.6887\n",
      "Epoch 5/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9626 - decoded_loss: 0.2726 - label_output_loss: 0.6900 - val_loss: 0.7603 - val_decoded_loss: 0.0719 - val_label_output_loss: 0.6884\n",
      "Epoch 6/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9494 - decoded_loss: 0.2598 - label_output_loss: 0.6896 - val_loss: 0.7559 - val_decoded_loss: 0.0674 - val_label_output_loss: 0.6885\n",
      "Epoch 7/1002\n",
      "87/87 [==============================] - 2s 24ms/step - loss: 0.9355 - decoded_loss: 0.2461 - label_output_loss: 0.6894 - val_loss: 0.7527 - val_decoded_loss: 0.0643 - val_label_output_loss: 0.6884\n",
      "Epoch 8/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9333 - decoded_loss: 0.2440 - label_output_loss: 0.6893 - val_loss: 0.7496 - val_decoded_loss: 0.0614 - val_label_output_loss: 0.6882\n",
      "Epoch 9/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.9335 - decoded_loss: 0.2444 - label_output_loss: 0.6892 - val_loss: 0.7462 - val_decoded_loss: 0.0579 - val_label_output_loss: 0.6884\n",
      "Epoch 10/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9290 - decoded_loss: 0.2399 - label_output_loss: 0.6891 - val_loss: 0.7443 - val_decoded_loss: 0.0560 - val_label_output_loss: 0.6883\n",
      "Epoch 11/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9220 - decoded_loss: 0.2331 - label_output_loss: 0.6889 - val_loss: 0.7433 - val_decoded_loss: 0.0553 - val_label_output_loss: 0.6880\n",
      "Epoch 12/1002\n",
      "87/87 [==============================] - 2s 23ms/step - loss: 0.9168 - decoded_loss: 0.2279 - label_output_loss: 0.6889 - val_loss: 0.7432 - val_decoded_loss: 0.0550 - val_label_output_loss: 0.6882\n",
      "Epoch 13/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.9148 - decoded_loss: 0.2260 - label_output_loss: 0.6888 - val_loss: 0.7381 - val_decoded_loss: 0.0497 - val_label_output_loss: 0.6884\n",
      "Epoch 14/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9158 - decoded_loss: 0.2271 - label_output_loss: 0.6887 - val_loss: 0.7396 - val_decoded_loss: 0.0517 - val_label_output_loss: 0.6879\n",
      "Epoch 15/1002\n",
      "87/87 [==============================] - 2s 25ms/step - loss: 0.9168 - decoded_loss: 0.2282 - label_output_loss: 0.6886 - val_loss: 0.7395 - val_decoded_loss: 0.0514 - val_label_output_loss: 0.6881\n",
      "Epoch 16/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9138 - decoded_loss: 0.2253 - label_output_loss: 0.6885 - val_loss: 0.7381 - val_decoded_loss: 0.0501 - val_label_output_loss: 0.6880\n",
      "Epoch 17/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9143 - decoded_loss: 0.2258 - label_output_loss: 0.6885 - val_loss: 0.7379 - val_decoded_loss: 0.0501 - val_label_output_loss: 0.6878\n",
      "Epoch 18/1002\n",
      "87/87 [==============================] - 2s 24ms/step - loss: 0.9110 - decoded_loss: 0.2226 - label_output_loss: 0.6884 - val_loss: 0.7420 - val_decoded_loss: 0.0538 - val_label_output_loss: 0.6883\n",
      "Epoch 19/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9166 - decoded_loss: 0.2282 - label_output_loss: 0.6884 - val_loss: 0.7358 - val_decoded_loss: 0.0478 - val_label_output_loss: 0.6880\n",
      "Epoch 20/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9092 - decoded_loss: 0.2209 - label_output_loss: 0.6883 - val_loss: 0.7376 - val_decoded_loss: 0.0496 - val_label_output_loss: 0.6880\n",
      "Epoch 21/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.9087 - decoded_loss: 0.2205 - label_output_loss: 0.6882 - val_loss: 0.7343 - val_decoded_loss: 0.0463 - val_label_output_loss: 0.6881\n",
      "Epoch 22/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.9079 - decoded_loss: 0.2197 - label_output_loss: 0.6882 - val_loss: 0.7372 - val_decoded_loss: 0.0493 - val_label_output_loss: 0.6879\n",
      "Epoch 23/1002\n",
      "87/87 [==============================] - 2s 25ms/step - loss: 0.9063 - decoded_loss: 0.2182 - label_output_loss: 0.6881 - val_loss: 0.7358 - val_decoded_loss: 0.0478 - val_label_output_loss: 0.6880\n",
      "Epoch 24/1002\n",
      "87/87 [==============================] - 2s 26ms/step - loss: 0.9085 - decoded_loss: 0.2204 - label_output_loss: 0.6881 - val_loss: 0.7367 - val_decoded_loss: 0.0485 - val_label_output_loss: 0.6882\n",
      "Epoch 25/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.9080 - decoded_loss: 0.2201 - label_output_loss: 0.6880 - val_loss: 0.7349 - val_decoded_loss: 0.0468 - val_label_output_loss: 0.6880\n",
      "Epoch 26/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.9083 - decoded_loss: 0.2203 - label_output_loss: 0.6879 - val_loss: 0.7345 - val_decoded_loss: 0.0464 - val_label_output_loss: 0.6881\n",
      "Epoch 27/1002\n",
      "87/87 [==============================] - 2s 24ms/step - loss: 0.9059 - decoded_loss: 0.2180 - label_output_loss: 0.6879 - val_loss: 0.7340 - val_decoded_loss: 0.0456 - val_label_output_loss: 0.6884\n",
      "Epoch 28/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.9058 - decoded_loss: 0.2179 - label_output_loss: 0.6879 - val_loss: 0.7365 - val_decoded_loss: 0.0483 - val_label_output_loss: 0.6882\n",
      "Epoch 29/1002\n",
      "87/87 [==============================] - 2s 24ms/step - loss: 0.9025 - decoded_loss: 0.2148 - label_output_loss: 0.6878 - val_loss: 0.7325 - val_decoded_loss: 0.0442 - val_label_output_loss: 0.6883\n",
      "Epoch 30/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.9072 - decoded_loss: 0.2194 - label_output_loss: 0.6878 - val_loss: 0.7352 - val_decoded_loss: 0.0471 - val_label_output_loss: 0.6881\n",
      "Epoch 31/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.9060 - decoded_loss: 0.2184 - label_output_loss: 0.6877 - val_loss: 0.7337 - val_decoded_loss: 0.0454 - val_label_output_loss: 0.6883\n",
      "Epoch 32/1002\n",
      "87/87 [==============================] - 2s 23ms/step - loss: 0.8977 - decoded_loss: 0.2102 - label_output_loss: 0.6876 - val_loss: 0.7325 - val_decoded_loss: 0.0445 - val_label_output_loss: 0.6880\n",
      "Epoch 33/1002\n",
      "87/87 [==============================] - 2s 23ms/step - loss: 0.8987 - decoded_loss: 0.2110 - label_output_loss: 0.6876 - val_loss: 0.7311 - val_decoded_loss: 0.0431 - val_label_output_loss: 0.6880\n",
      "Epoch 34/1002\n",
      "87/87 [==============================] - 2s 23ms/step - loss: 0.8988 - decoded_loss: 0.2113 - label_output_loss: 0.6875 - val_loss: 0.7326 - val_decoded_loss: 0.0447 - val_label_output_loss: 0.6879\n",
      "Epoch 35/1002\n",
      "87/87 [==============================] - 2s 23ms/step - loss: 0.9070 - decoded_loss: 0.2195 - label_output_loss: 0.6875 - val_loss: 0.7337 - val_decoded_loss: 0.0456 - val_label_output_loss: 0.6882\n",
      "Epoch 36/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.8971 - decoded_loss: 0.2096 - label_output_loss: 0.6874 - val_loss: 0.7326 - val_decoded_loss: 0.0441 - val_label_output_loss: 0.6885\n",
      "Epoch 37/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.8988 - decoded_loss: 0.2114 - label_output_loss: 0.6874 - val_loss: 0.7343 - val_decoded_loss: 0.0459 - val_label_output_loss: 0.6884\n",
      "Epoch 38/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.8988 - decoded_loss: 0.2115 - label_output_loss: 0.6873 - val_loss: 0.7318 - val_decoded_loss: 0.0438 - val_label_output_loss: 0.6879\n",
      "Epoch 39/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.8967 - decoded_loss: 0.2095 - label_output_loss: 0.6872 - val_loss: 0.7291 - val_decoded_loss: 0.0408 - val_label_output_loss: 0.6883\n",
      "Epoch 40/1002\n",
      "87/87 [==============================] - 2s 24ms/step - loss: 0.8973 - decoded_loss: 0.2101 - label_output_loss: 0.6873 - val_loss: 0.7343 - val_decoded_loss: 0.0461 - val_label_output_loss: 0.6882\n",
      "Epoch 41/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.8962 - decoded_loss: 0.2090 - label_output_loss: 0.6872 - val_loss: 0.7291 - val_decoded_loss: 0.0407 - val_label_output_loss: 0.6884\n",
      "Epoch 42/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.8973 - decoded_loss: 0.2101 - label_output_loss: 0.6872 - val_loss: 0.7307 - val_decoded_loss: 0.0426 - val_label_output_loss: 0.6882\n",
      "Epoch 43/1002\n",
      "87/87 [==============================] - 2s 23ms/step - loss: 0.9012 - decoded_loss: 0.2142 - label_output_loss: 0.6870 - val_loss: 0.7302 - val_decoded_loss: 0.0421 - val_label_output_loss: 0.6881\n",
      "Epoch 44/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.9027 - decoded_loss: 0.2156 - label_output_loss: 0.6871 - val_loss: 0.7320 - val_decoded_loss: 0.0437 - val_label_output_loss: 0.6883\n",
      "Epoch 45/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.8959 - decoded_loss: 0.2089 - label_output_loss: 0.6870 - val_loss: 0.7297 - val_decoded_loss: 0.0417 - val_label_output_loss: 0.6881\n",
      "Epoch 46/1002\n",
      "87/87 [==============================] - 2s 24ms/step - loss: 0.8900 - decoded_loss: 0.2031 - label_output_loss: 0.6869 - val_loss: 0.7303 - val_decoded_loss: 0.0424 - val_label_output_loss: 0.6879\n",
      "Epoch 47/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.8935 - decoded_loss: 0.2066 - label_output_loss: 0.6869 - val_loss: 0.7273 - val_decoded_loss: 0.0392 - val_label_output_loss: 0.6881\n",
      "Epoch 48/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.8975 - decoded_loss: 0.2107 - label_output_loss: 0.6869 - val_loss: 0.7273 - val_decoded_loss: 0.0390 - val_label_output_loss: 0.6883\n",
      "Epoch 49/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.8932 - decoded_loss: 0.2063 - label_output_loss: 0.6868 - val_loss: 0.7305 - val_decoded_loss: 0.0421 - val_label_output_loss: 0.6884\n",
      "Epoch 50/1002\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 0.8992 - decoded_loss: 0.2124 - label_output_loss: 0.6868 - val_loss: 0.7297 - val_decoded_loss: 0.0416 - val_label_output_loss: 0.6881\n",
      "Epoch 51/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.8893 - decoded_loss: 0.2026 - label_output_loss: 0.6867 - val_loss: 0.7307 - val_decoded_loss: 0.0422 - val_label_output_loss: 0.6885\n",
      "Epoch 52/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.8925 - decoded_loss: 0.2058 - label_output_loss: 0.6867 - val_loss: 0.7316 - val_decoded_loss: 0.0432 - val_label_output_loss: 0.6883\n",
      "Epoch 53/1002\n",
      "87/87 [==============================] - 2s 23ms/step - loss: 0.8917 - decoded_loss: 0.2051 - label_output_loss: 0.6866 - val_loss: 0.7287 - val_decoded_loss: 0.0405 - val_label_output_loss: 0.6882\n",
      "Epoch 54/1002\n",
      "87/87 [==============================] - 2s 23ms/step - loss: 0.8953 - decoded_loss: 0.2086 - label_output_loss: 0.6866 - val_loss: 0.7298 - val_decoded_loss: 0.0413 - val_label_output_loss: 0.6885\n",
      "Epoch 55/1002\n",
      "87/87 [==============================] - 2s 22ms/step - loss: 0.8853 - decoded_loss: 0.1988 - label_output_loss: 0.6865 - val_loss: 0.7310 - val_decoded_loss: 0.0426 - val_label_output_loss: 0.6884\n",
      "Epoch 56/1002\n",
      "87/87 [==============================] - 2s 23ms/step - loss: 0.8936 - decoded_loss: 0.2071 - label_output_loss: 0.6865 - val_loss: 0.7283 - val_decoded_loss: 0.0395 - val_label_output_loss: 0.6888\n",
      "Epoch 57/1002\n",
      "87/87 [==============================] - 2s 26ms/step - loss: 0.8951 - decoded_loss: 0.2087 - label_output_loss: 0.6864 - val_loss: 0.7312 - val_decoded_loss: 0.0429 - val_label_output_loss: 0.6883\n"
     ]
    }
   ],
   "source": [
    "# train autoencoder once, save and reuse.....\n",
    "\n",
    "autoencoder, encoder = create_autoencoder(X.shape[-1], y.shape[-1], noise=0.1)\n",
    "if TRAINING:\n",
    "    autoencoder.fit(X,(X,y),\n",
    "                    epochs=1002,\n",
    "                    batch_size=16384, \n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping('val_loss', patience=10,restore_best_weights=True)])\n",
    "    encoder.save_weights('encoder.hdf5')\n",
    "    encoder.save('saved_model_encoder.hdf5')\n",
    "else:\n",
    "    encoder.load_weights('encoder.hdf5')\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:43:17.249466Z",
     "iopub.status.busy": "2020-12-31T16:43:17.248684Z",
     "iopub.status.idle": "2020-12-31T16:43:17.252926Z",
     "shell.execute_reply": "2020-12-31T16:43:17.253980Z"
    },
    "papermill": {
     "duration": 0.829369,
     "end_time": "2020-12-31T16:43:17.254134",
     "exception": false,
     "start_time": "2020-12-31T16:43:16.424765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode inputs using trained bottleneck encoder\n",
    "# concat encoded and raw input data\n",
    "# normalize layer, dropout\n",
    "# dense layer, batch normalization, dropout - swish is a smoothed leaky relu\n",
    "# 5 targets - all return info provided *** not sure sigmoid is correct activation here, try mse\n",
    "# *** add label smoothing?\n",
    "\n",
    "def create_model(input_dim, output_dim, encoder, lr=0.0001):\n",
    "\n",
    "    inputs = Input(shape=(input_dim, ))\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = Concatenate()([x, inputs])         # use both raw and encoded features\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # ?  up to 5 repeats    \n",
    "    x = Dense(256, activation='swish')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(output_dim, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # ? add label smoothing\n",
    "    model.compile(Adam(learning_rate=lr), \n",
    "                  loss=BinaryCrossentropy(), \n",
    "                  metrics=[tf.keras.metrics.AUC(name = 'auc')])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:43:18.482699Z",
     "iopub.status.busy": "2020-12-31T16:43:18.481820Z",
     "iopub.status.idle": "2020-12-31T16:53:31.303291Z",
     "shell.execute_reply": "2020-12-31T16:53:31.301929Z"
    },
    "papermill": {
     "duration": 613.343467,
     "end_time": "2020-12-31T16:53:31.303404",
     "exception": false,
     "start_time": "2020-12-31T16:43:17.959937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train idx, test idx [     0      1      2 ... 261902 261903 261904] [261905 261906 261907 ... 523804 523805 523806]\n",
      "train/test (261905, 130) (261902, 130) (261905, 5) (261902, 5)\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.9206 - auc: 0.5049 - val_loss: 0.7105 - val_auc: 0.5158\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.8578 - auc: 0.5077 - val_loss: 0.7035 - val_auc: 0.5191\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.8213 - auc: 0.5089 - val_loss: 0.6989 - val_auc: 0.5222\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.8005 - auc: 0.5103 - val_loss: 0.6966 - val_auc: 0.5254\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7852 - auc: 0.5113 - val_loss: 0.6948 - val_auc: 0.5268\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7726 - auc: 0.5117 - val_loss: 0.6941 - val_auc: 0.5285\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 1s 15ms/step - loss: 0.7613 - auc: 0.5135 - val_loss: 0.6934 - val_auc: 0.5289\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7537 - auc: 0.5139 - val_loss: 0.6927 - val_auc: 0.5306\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7481 - auc: 0.5144 - val_loss: 0.6925 - val_auc: 0.5309\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7434 - auc: 0.5144 - val_loss: 0.6923 - val_auc: 0.5315\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7372 - auc: 0.5152 - val_loss: 0.6921 - val_auc: 0.5323\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7341 - auc: 0.5163 - val_loss: 0.6920 - val_auc: 0.5327\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7297 - auc: 0.5160 - val_loss: 0.6919 - val_auc: 0.5329\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7269 - auc: 0.5168 - val_loss: 0.6919 - val_auc: 0.5333\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7247 - auc: 0.5177 - val_loss: 0.6917 - val_auc: 0.5338\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7216 - auc: 0.5184 - val_loss: 0.6917 - val_auc: 0.5338\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7194 - auc: 0.5198 - val_loss: 0.6918 - val_auc: 0.5339\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7176 - auc: 0.5189 - val_loss: 0.6916 - val_auc: 0.5343\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7160 - auc: 0.5208 - val_loss: 0.6916 - val_auc: 0.5344\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.7142 - auc: 0.5205 - val_loss: 0.6915 - val_auc: 0.5347\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 1s 14ms/step - loss: 0.7121 - auc: 0.5221 - val_loss: 0.6914 - val_auc: 0.5352\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7115 - auc: 0.5231 - val_loss: 0.6914 - val_auc: 0.5353\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7098 - auc: 0.5234 - val_loss: 0.6914 - val_auc: 0.5358\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7090 - auc: 0.5246 - val_loss: 0.6913 - val_auc: 0.5357\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7081 - auc: 0.5249 - val_loss: 0.6913 - val_auc: 0.5359\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7061 - auc: 0.5270 - val_loss: 0.6913 - val_auc: 0.5360\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7060 - auc: 0.5265 - val_loss: 0.6913 - val_auc: 0.5363\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7056 - auc: 0.5272 - val_loss: 0.6913 - val_auc: 0.5366\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7039 - auc: 0.5287 - val_loss: 0.6912 - val_auc: 0.5371\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7034 - auc: 0.5291 - val_loss: 0.6912 - val_auc: 0.5370\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7027 - auc: 0.5288 - val_loss: 0.6911 - val_auc: 0.5374\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7022 - auc: 0.5304 - val_loss: 0.6910 - val_auc: 0.5378\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.7012 - auc: 0.5319 - val_loss: 0.6910 - val_auc: 0.5382\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.7007 - auc: 0.5329 - val_loss: 0.6909 - val_auc: 0.5384\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.7008 - auc: 0.5325 - val_loss: 0.6910 - val_auc: 0.5383\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.6993 - auc: 0.5339 - val_loss: 0.6909 - val_auc: 0.5382\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 1s 14ms/step - loss: 0.6992 - auc: 0.5337 - val_loss: 0.6908 - val_auc: 0.5391\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6988 - auc: 0.5352 - val_loss: 0.6908 - val_auc: 0.5392\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6986 - auc: 0.5365 - val_loss: 0.6907 - val_auc: 0.5394\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6977 - auc: 0.5367 - val_loss: 0.6907 - val_auc: 0.5394\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6972 - auc: 0.5376 - val_loss: 0.6907 - val_auc: 0.5395\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6967 - auc: 0.5376 - val_loss: 0.6905 - val_auc: 0.5399\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 1s 16ms/step - loss: 0.6967 - auc: 0.5386 - val_loss: 0.6905 - val_auc: 0.5402\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6960 - auc: 0.5390 - val_loss: 0.6905 - val_auc: 0.5404\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6956 - auc: 0.5402 - val_loss: 0.6905 - val_auc: 0.5404\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6954 - auc: 0.5401 - val_loss: 0.6905 - val_auc: 0.5405\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6952 - auc: 0.5415 - val_loss: 0.6904 - val_auc: 0.5409\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 1s 15ms/step - loss: 0.6946 - auc: 0.5421 - val_loss: 0.6903 - val_auc: 0.5411\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6946 - auc: 0.5408 - val_loss: 0.6903 - val_auc: 0.5411\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6940 - auc: 0.5426 - val_loss: 0.6903 - val_auc: 0.5412\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6938 - auc: 0.5430 - val_loss: 0.6903 - val_auc: 0.5413\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6932 - auc: 0.5435 - val_loss: 0.6903 - val_auc: 0.5414\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6935 - auc: 0.5433 - val_loss: 0.6902 - val_auc: 0.5416\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6933 - auc: 0.5430 - val_loss: 0.6903 - val_auc: 0.5414\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6924 - auc: 0.5456 - val_loss: 0.6902 - val_auc: 0.5416\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6929 - auc: 0.5443 - val_loss: 0.6902 - val_auc: 0.5418\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6924 - auc: 0.5449 - val_loss: 0.6901 - val_auc: 0.5419\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6921 - auc: 0.5457 - val_loss: 0.6902 - val_auc: 0.5418\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 1s 15ms/step - loss: 0.6920 - auc: 0.5459 - val_loss: 0.6901 - val_auc: 0.5420\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 1s 16ms/step - loss: 0.6916 - auc: 0.5466 - val_loss: 0.6901 - val_auc: 0.5418\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.6914 - auc: 0.5471 - val_loss: 0.6901 - val_auc: 0.5421\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 1s 14ms/step - loss: 0.6915 - auc: 0.5466 - val_loss: 0.6901 - val_auc: 0.5419\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6912 - auc: 0.5473 - val_loss: 0.6900 - val_auc: 0.5422\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6911 - auc: 0.5474 - val_loss: 0.6900 - val_auc: 0.5423\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6909 - auc: 0.5472 - val_loss: 0.6901 - val_auc: 0.5423\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6908 - auc: 0.5484 - val_loss: 0.6900 - val_auc: 0.5422\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6905 - auc: 0.5479 - val_loss: 0.6900 - val_auc: 0.5424\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6904 - auc: 0.5486 - val_loss: 0.6900 - val_auc: 0.5422\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6903 - auc: 0.5489 - val_loss: 0.6900 - val_auc: 0.5425\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6902 - auc: 0.5480 - val_loss: 0.6900 - val_auc: 0.5424\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6900 - auc: 0.5491 - val_loss: 0.6900 - val_auc: 0.5426\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6900 - auc: 0.5494 - val_loss: 0.6899 - val_auc: 0.5428\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6898 - auc: 0.5495 - val_loss: 0.6900 - val_auc: 0.5424\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6894 - auc: 0.5504 - val_loss: 0.6899 - val_auc: 0.5426\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.6894 - auc: 0.5505 - val_loss: 0.6899 - val_auc: 0.5426\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 1s 14ms/step - loss: 0.6893 - auc: 0.5506 - val_loss: 0.6899 - val_auc: 0.5428\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6892 - auc: 0.5506 - val_loss: 0.6899 - val_auc: 0.5427\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6893 - auc: 0.5506 - val_loss: 0.6899 - val_auc: 0.5429\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6891 - auc: 0.5505 - val_loss: 0.6899 - val_auc: 0.5427\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6889 - auc: 0.5514 - val_loss: 0.6899 - val_auc: 0.5426\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6890 - auc: 0.5510 - val_loss: 0.6899 - val_auc: 0.5428\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6889 - auc: 0.5507 - val_loss: 0.6899 - val_auc: 0.5427\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.6887 - auc: 0.5514 - val_loss: 0.6899 - val_auc: 0.5427\n",
      "Epoch 1/3\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.6934 - auc: 0.5335\n",
      "Epoch 2/3\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.6932 - auc: 0.5344\n",
      "Epoch 3/3\n",
      "64/64 [==============================] - 1s 10ms/step - loss: 0.6929 - auc: 0.5348\n",
      "train idx, test idx [     0      1      2 ... 523804 523805 523806] [523807 523808 523809 ... 785706 785707 785708]\n",
      "train/test (523807, 130) (261902, 130) (523807, 5) (261902, 5)\n",
      "Epoch 1/100\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.9008 - auc: 0.5047 - val_loss: 0.7124 - val_auc: 0.5200\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.8247 - auc: 0.5084 - val_loss: 0.7030 - val_auc: 0.5264\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7887 - auc: 0.5099 - val_loss: 0.6969 - val_auc: 0.5301\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7673 - auc: 0.5106 - val_loss: 0.6938 - val_auc: 0.5322\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7531 - auc: 0.5115 - val_loss: 0.6926 - val_auc: 0.5348\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7426 - auc: 0.5134 - val_loss: 0.6918 - val_auc: 0.5364\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7341 - auc: 0.5148 - val_loss: 0.6916 - val_auc: 0.5377\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.7276 - auc: 0.5157 - val_loss: 0.6912 - val_auc: 0.5386\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7225 - auc: 0.5162 - val_loss: 0.6910 - val_auc: 0.5400\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7176 - auc: 0.5181 - val_loss: 0.6910 - val_auc: 0.5405\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7143 - auc: 0.5189 - val_loss: 0.6907 - val_auc: 0.5411\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7106 - auc: 0.5209 - val_loss: 0.6906 - val_auc: 0.5414\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7081 - auc: 0.5220 - val_loss: 0.6905 - val_auc: 0.5420\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7056 - auc: 0.5231 - val_loss: 0.6905 - val_auc: 0.5423\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.7038 - auc: 0.5256 - val_loss: 0.6904 - val_auc: 0.5427\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.7018 - auc: 0.5268 - val_loss: 0.6904 - val_auc: 0.5427\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.7003 - auc: 0.5286 - val_loss: 0.6903 - val_auc: 0.5432\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6990 - auc: 0.5295 - val_loss: 0.6902 - val_auc: 0.5435\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.6979 - auc: 0.5312 - val_loss: 0.6901 - val_auc: 0.5436\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6965 - auc: 0.5325 - val_loss: 0.6901 - val_auc: 0.5439\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6958 - auc: 0.5339 - val_loss: 0.6900 - val_auc: 0.5440\n",
      "Epoch 22/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6947 - auc: 0.5351 - val_loss: 0.6900 - val_auc: 0.5441\n",
      "Epoch 23/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6941 - auc: 0.5367 - val_loss: 0.6899 - val_auc: 0.5444\n",
      "Epoch 24/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6936 - auc: 0.5370 - val_loss: 0.6899 - val_auc: 0.5444\n",
      "Epoch 25/100\n",
      "128/128 [==============================] - 2s 13ms/step - loss: 0.6930 - auc: 0.5376 - val_loss: 0.6898 - val_auc: 0.5448\n",
      "Epoch 26/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6925 - auc: 0.5386 - val_loss: 0.6898 - val_auc: 0.5448\n",
      "Epoch 27/100\n",
      "128/128 [==============================] - 1s 12ms/step - loss: 0.6921 - auc: 0.5398 - val_loss: 0.6898 - val_auc: 0.5449\n",
      "Epoch 28/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.6917 - auc: 0.5405 - val_loss: 0.6897 - val_auc: 0.5452\n",
      "Epoch 29/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.6914 - auc: 0.5410 - val_loss: 0.6897 - val_auc: 0.5453\n",
      "Epoch 30/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6912 - auc: 0.5413 - val_loss: 0.6897 - val_auc: 0.5453\n",
      "Epoch 31/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6909 - auc: 0.5416 - val_loss: 0.6897 - val_auc: 0.5449\n",
      "Epoch 32/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6906 - auc: 0.5426 - val_loss: 0.6896 - val_auc: 0.5456\n",
      "Epoch 33/100\n",
      "128/128 [==============================] - 1s 12ms/step - loss: 0.6906 - auc: 0.5423 - val_loss: 0.6896 - val_auc: 0.5458\n",
      "Epoch 34/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6903 - auc: 0.5433 - val_loss: 0.6895 - val_auc: 0.5458\n",
      "Epoch 35/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6902 - auc: 0.5438 - val_loss: 0.6896 - val_auc: 0.5457\n",
      "Epoch 36/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6899 - auc: 0.5445 - val_loss: 0.6895 - val_auc: 0.5461\n",
      "Epoch 37/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6899 - auc: 0.5444 - val_loss: 0.6895 - val_auc: 0.5458\n",
      "Epoch 38/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6898 - auc: 0.5451 - val_loss: 0.6895 - val_auc: 0.5460\n",
      "Epoch 39/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6899 - auc: 0.5443 - val_loss: 0.6894 - val_auc: 0.5462\n",
      "Epoch 40/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.6897 - auc: 0.5454 - val_loss: 0.6895 - val_auc: 0.5460\n",
      "Epoch 41/100\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.6896 - auc: 0.5452 - val_loss: 0.6894 - val_auc: 0.5462\n",
      "Epoch 42/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.6895 - auc: 0.5454 - val_loss: 0.6894 - val_auc: 0.5463\n",
      "Epoch 43/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6895 - auc: 0.5458 - val_loss: 0.6894 - val_auc: 0.5463\n",
      "Epoch 44/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6894 - auc: 0.5462 - val_loss: 0.6894 - val_auc: 0.5464\n",
      "Epoch 45/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6893 - auc: 0.5468 - val_loss: 0.6893 - val_auc: 0.5467\n",
      "Epoch 46/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6894 - auc: 0.5464 - val_loss: 0.6893 - val_auc: 0.5467\n",
      "Epoch 47/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6893 - auc: 0.5467 - val_loss: 0.6893 - val_auc: 0.5468\n",
      "Epoch 48/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6893 - auc: 0.5460 - val_loss: 0.6893 - val_auc: 0.5469\n",
      "Epoch 49/100\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.6893 - auc: 0.5463 - val_loss: 0.6893 - val_auc: 0.5467\n",
      "Epoch 50/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.6892 - auc: 0.5470 - val_loss: 0.6893 - val_auc: 0.5469\n",
      "Epoch 51/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6891 - auc: 0.5478 - val_loss: 0.6892 - val_auc: 0.5469\n",
      "Epoch 52/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6891 - auc: 0.5478 - val_loss: 0.6892 - val_auc: 0.5469\n",
      "Epoch 53/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6891 - auc: 0.5477 - val_loss: 0.6892 - val_auc: 0.5471\n",
      "Epoch 54/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.6890 - auc: 0.5483 - val_loss: 0.6893 - val_auc: 0.5467\n",
      "Epoch 55/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.6891 - auc: 0.5475 - val_loss: 0.6892 - val_auc: 0.5470\n",
      "Epoch 56/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6891 - auc: 0.5479 - val_loss: 0.6892 - val_auc: 0.5471\n",
      "Epoch 57/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6889 - auc: 0.5483 - val_loss: 0.6892 - val_auc: 0.5473\n",
      "Epoch 58/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.6889 - auc: 0.5483 - val_loss: 0.6891 - val_auc: 0.5475\n",
      "Epoch 59/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6888 - auc: 0.5490 - val_loss: 0.6891 - val_auc: 0.5473\n",
      "Epoch 60/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6888 - auc: 0.5490 - val_loss: 0.6891 - val_auc: 0.5472\n",
      "Epoch 61/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6887 - auc: 0.5493 - val_loss: 0.6891 - val_auc: 0.5475\n",
      "Epoch 62/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6888 - auc: 0.5491 - val_loss: 0.6891 - val_auc: 0.5475\n",
      "Epoch 63/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6887 - auc: 0.5496 - val_loss: 0.6891 - val_auc: 0.5475\n",
      "Epoch 1/3\n",
      "64/64 [==============================] - 1s 9ms/step - loss: 0.6903 - auc: 0.5411\n",
      "Epoch 2/3\n",
      "64/64 [==============================] - 1s 8ms/step - loss: 0.6903 - auc: 0.5409\n",
      "Epoch 3/3\n",
      "64/64 [==============================] - 1s 9ms/step - loss: 0.6902 - auc: 0.5417\n",
      "train idx, test idx [     0      1      2 ... 785706 785707 785708] [ 785709  785710  785711 ... 1047608 1047609 1047610]\n",
      "train/test (785709, 130) (261902, 130) (785709, 5) (261902, 5)\n",
      "Epoch 1/100\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.8808 - auc: 0.5053 - val_loss: 0.7026 - val_auc: 0.5190\n",
      "Epoch 2/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.8010 - auc: 0.5087 - val_loss: 0.6955 - val_auc: 0.5245\n",
      "Epoch 3/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.7674 - auc: 0.5102 - val_loss: 0.6930 - val_auc: 0.5275\n",
      "Epoch 4/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.7492 - auc: 0.5111 - val_loss: 0.6922 - val_auc: 0.5301\n",
      "Epoch 5/100\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.7353 - auc: 0.5134 - val_loss: 0.6916 - val_auc: 0.5316\n",
      "Epoch 6/100\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.7255 - auc: 0.5149 - val_loss: 0.6915 - val_auc: 0.5328\n",
      "Epoch 7/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.7184 - auc: 0.5160 - val_loss: 0.6912 - val_auc: 0.5342\n",
      "Epoch 8/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.7126 - auc: 0.5175 - val_loss: 0.6911 - val_auc: 0.5353\n",
      "Epoch 9/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.7077 - auc: 0.5202 - val_loss: 0.6909 - val_auc: 0.5362\n",
      "Epoch 10/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.7038 - auc: 0.5222 - val_loss: 0.6907 - val_auc: 0.5371\n",
      "Epoch 11/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.7007 - auc: 0.5253 - val_loss: 0.6906 - val_auc: 0.5378\n",
      "Epoch 12/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6981 - auc: 0.5279 - val_loss: 0.6905 - val_auc: 0.5383\n",
      "Epoch 13/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6962 - auc: 0.5297 - val_loss: 0.6903 - val_auc: 0.5393\n",
      "Epoch 14/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6946 - auc: 0.5323 - val_loss: 0.6903 - val_auc: 0.5394\n",
      "Epoch 15/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6935 - auc: 0.5336 - val_loss: 0.6902 - val_auc: 0.5399\n",
      "Epoch 16/100\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.6924 - auc: 0.5359 - val_loss: 0.6901 - val_auc: 0.5398\n",
      "Epoch 17/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6917 - auc: 0.5374 - val_loss: 0.6901 - val_auc: 0.5402\n",
      "Epoch 18/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6912 - auc: 0.5383 - val_loss: 0.6900 - val_auc: 0.5404\n",
      "Epoch 19/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6909 - auc: 0.5392 - val_loss: 0.6900 - val_auc: 0.5409\n",
      "Epoch 20/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6906 - auc: 0.5404 - val_loss: 0.6899 - val_auc: 0.5411\n",
      "Epoch 21/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6905 - auc: 0.5407 - val_loss: 0.6899 - val_auc: 0.5411\n",
      "Epoch 22/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6903 - auc: 0.5413 - val_loss: 0.6899 - val_auc: 0.5415\n",
      "Epoch 23/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6902 - auc: 0.5415 - val_loss: 0.6898 - val_auc: 0.5417\n",
      "Epoch 24/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6901 - auc: 0.5421 - val_loss: 0.6898 - val_auc: 0.5418\n",
      "Epoch 25/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6900 - auc: 0.5426 - val_loss: 0.6898 - val_auc: 0.5418\n",
      "Epoch 26/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6899 - auc: 0.5427 - val_loss: 0.6897 - val_auc: 0.5422\n",
      "Epoch 27/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6898 - auc: 0.5432 - val_loss: 0.6897 - val_auc: 0.5421\n",
      "Epoch 28/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6899 - auc: 0.5432 - val_loss: 0.6897 - val_auc: 0.5424\n",
      "Epoch 29/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6897 - auc: 0.5436 - val_loss: 0.6897 - val_auc: 0.5426\n",
      "Epoch 30/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6896 - auc: 0.5444 - val_loss: 0.6896 - val_auc: 0.5427\n",
      "Epoch 31/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6896 - auc: 0.5446 - val_loss: 0.6896 - val_auc: 0.5430\n",
      "Epoch 32/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6896 - auc: 0.5446 - val_loss: 0.6895 - val_auc: 0.5431\n",
      "Epoch 33/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6895 - auc: 0.5451 - val_loss: 0.6895 - val_auc: 0.5432\n",
      "Epoch 34/100\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.6894 - auc: 0.5456 - val_loss: 0.6895 - val_auc: 0.5434\n",
      "Epoch 35/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6894 - auc: 0.5459 - val_loss: 0.6895 - val_auc: 0.5434\n",
      "Epoch 36/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6894 - auc: 0.5460 - val_loss: 0.6895 - val_auc: 0.5430\n",
      "Epoch 37/100\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.6893 - auc: 0.5463 - val_loss: 0.6895 - val_auc: 0.5435\n",
      "Epoch 38/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6893 - auc: 0.5463 - val_loss: 0.6894 - val_auc: 0.5437\n",
      "Epoch 39/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6893 - auc: 0.5465 - val_loss: 0.6894 - val_auc: 0.5439\n",
      "Epoch 40/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6892 - auc: 0.5468 - val_loss: 0.6894 - val_auc: 0.5439\n",
      "Epoch 41/100\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.6890 - auc: 0.5476 - val_loss: 0.6894 - val_auc: 0.5439\n",
      "Epoch 42/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6891 - auc: 0.5473 - val_loss: 0.6893 - val_auc: 0.5441\n",
      "Epoch 43/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6891 - auc: 0.5474 - val_loss: 0.6893 - val_auc: 0.5439\n",
      "Epoch 44/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6890 - auc: 0.5477 - val_loss: 0.6893 - val_auc: 0.5442\n",
      "Epoch 45/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6889 - auc: 0.5482 - val_loss: 0.6893 - val_auc: 0.5441\n",
      "Epoch 46/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6890 - auc: 0.5478 - val_loss: 0.6893 - val_auc: 0.5445\n",
      "Epoch 47/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6889 - auc: 0.5484 - val_loss: 0.6893 - val_auc: 0.5442\n",
      "Epoch 48/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6889 - auc: 0.5483 - val_loss: 0.6893 - val_auc: 0.5442\n",
      "Epoch 49/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6888 - auc: 0.5487 - val_loss: 0.6893 - val_auc: 0.5443\n",
      "Epoch 50/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6888 - auc: 0.5488 - val_loss: 0.6893 - val_auc: 0.5444\n",
      "Epoch 51/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6887 - auc: 0.5493 - val_loss: 0.6892 - val_auc: 0.5447\n",
      "Epoch 52/100\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.6888 - auc: 0.5488 - val_loss: 0.6893 - val_auc: 0.5444\n",
      "Epoch 53/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6887 - auc: 0.5495 - val_loss: 0.6893 - val_auc: 0.5444\n",
      "Epoch 54/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6887 - auc: 0.5499 - val_loss: 0.6892 - val_auc: 0.5446\n",
      "Epoch 55/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6886 - auc: 0.5497 - val_loss: 0.6892 - val_auc: 0.5448\n",
      "Epoch 56/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6886 - auc: 0.5499 - val_loss: 0.6892 - val_auc: 0.5448\n",
      "Epoch 57/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6885 - auc: 0.5502 - val_loss: 0.6892 - val_auc: 0.5445\n",
      "Epoch 58/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6885 - auc: 0.5502 - val_loss: 0.6892 - val_auc: 0.5448\n",
      "Epoch 59/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6885 - auc: 0.5507 - val_loss: 0.6891 - val_auc: 0.5450\n",
      "Epoch 60/100\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.6885 - auc: 0.5507 - val_loss: 0.6892 - val_auc: 0.5449\n",
      "Epoch 61/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6884 - auc: 0.5506 - val_loss: 0.6891 - val_auc: 0.5448\n",
      "Epoch 62/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6884 - auc: 0.5508 - val_loss: 0.6892 - val_auc: 0.5448\n",
      "Epoch 63/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6884 - auc: 0.5512 - val_loss: 0.6892 - val_auc: 0.5447\n",
      "Epoch 64/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.6883 - auc: 0.5512 - val_loss: 0.6892 - val_auc: 0.5447\n",
      "Epoch 1/3\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.6905 - auc: 0.5384\n",
      "Epoch 2/3\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.6904 - auc: 0.5388\n",
      "Epoch 3/3\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.6902 - auc: 0.5395\n",
      "train idx, test idx [      0       1       2 ... 1047608 1047609 1047610] [1047611 1047612 1047613 ... 1309510 1309511 1309512]\n",
      "train/test (1047611, 130) (261902, 130) (1047611, 5) (261902, 5)\n",
      "Epoch 1/100\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.8683 - auc: 0.5063 - val_loss: 0.7031 - val_auc: 0.5210\n",
      "Epoch 2/100\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.7845 - auc: 0.5092 - val_loss: 0.6946 - val_auc: 0.5269\n",
      "Epoch 3/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.7523 - auc: 0.5114 - val_loss: 0.6927 - val_auc: 0.5304\n",
      "Epoch 4/100\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.7341 - auc: 0.5127 - val_loss: 0.6918 - val_auc: 0.5330\n",
      "Epoch 5/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.7219 - auc: 0.5148 - val_loss: 0.6916 - val_auc: 0.5344\n",
      "Epoch 6/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.7135 - auc: 0.5164 - val_loss: 0.6913 - val_auc: 0.5354\n",
      "Epoch 7/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.7064 - auc: 0.5202 - val_loss: 0.6910 - val_auc: 0.5372\n",
      "Epoch 8/100\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.7017 - auc: 0.5226 - val_loss: 0.6909 - val_auc: 0.5379\n",
      "Epoch 9/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6978 - auc: 0.5264 - val_loss: 0.6907 - val_auc: 0.5384\n",
      "Epoch 10/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6951 - auc: 0.5298 - val_loss: 0.6905 - val_auc: 0.5392\n",
      "Epoch 11/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6933 - auc: 0.5323 - val_loss: 0.6903 - val_auc: 0.5397\n",
      "Epoch 12/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6920 - auc: 0.5350 - val_loss: 0.6902 - val_auc: 0.5400\n",
      "Epoch 13/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6913 - auc: 0.5362 - val_loss: 0.6903 - val_auc: 0.5399\n",
      "Epoch 14/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6909 - auc: 0.5378 - val_loss: 0.6902 - val_auc: 0.5395\n",
      "Epoch 15/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6906 - auc: 0.5388 - val_loss: 0.6901 - val_auc: 0.5400\n",
      "Epoch 16/100\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 0.6904 - auc: 0.5396 - val_loss: 0.6901 - val_auc: 0.5403\n",
      "Epoch 17/100\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 0.6903 - auc: 0.5398 - val_loss: 0.6901 - val_auc: 0.5401\n",
      "Epoch 18/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6902 - auc: 0.5403 - val_loss: 0.6901 - val_auc: 0.5401\n",
      "Epoch 19/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6901 - auc: 0.5410 - val_loss: 0.6900 - val_auc: 0.5405\n",
      "Epoch 20/100\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 0.6900 - auc: 0.5416 - val_loss: 0.6900 - val_auc: 0.5405\n",
      "Epoch 21/100\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 0.6900 - auc: 0.5415 - val_loss: 0.6900 - val_auc: 0.5411\n",
      "Epoch 22/100\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.6899 - auc: 0.5421 - val_loss: 0.6900 - val_auc: 0.5410\n",
      "Epoch 23/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6899 - auc: 0.5424 - val_loss: 0.6899 - val_auc: 0.5414\n",
      "Epoch 24/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6898 - auc: 0.5429 - val_loss: 0.6899 - val_auc: 0.5416\n",
      "Epoch 25/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6898 - auc: 0.5430 - val_loss: 0.6899 - val_auc: 0.5414\n",
      "Epoch 26/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6897 - auc: 0.5431 - val_loss: 0.6898 - val_auc: 0.5416\n",
      "Epoch 27/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6897 - auc: 0.5436 - val_loss: 0.6898 - val_auc: 0.5422\n",
      "Epoch 28/100\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.6896 - auc: 0.5442 - val_loss: 0.6898 - val_auc: 0.5421\n",
      "Epoch 29/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6895 - auc: 0.5447 - val_loss: 0.6898 - val_auc: 0.5422\n",
      "Epoch 30/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6895 - auc: 0.5445 - val_loss: 0.6898 - val_auc: 0.5422\n",
      "Epoch 31/100\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.6894 - auc: 0.5452 - val_loss: 0.6898 - val_auc: 0.5422\n",
      "Epoch 32/100\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 0.6894 - auc: 0.5448 - val_loss: 0.6897 - val_auc: 0.5425\n",
      "Epoch 33/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6893 - auc: 0.5457 - val_loss: 0.6897 - val_auc: 0.5428\n",
      "Epoch 34/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6893 - auc: 0.5452 - val_loss: 0.6896 - val_auc: 0.5429\n",
      "Epoch 35/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6893 - auc: 0.5456 - val_loss: 0.6896 - val_auc: 0.5430\n",
      "Epoch 36/100\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.6892 - auc: 0.5461 - val_loss: 0.6897 - val_auc: 0.5426\n",
      "Epoch 37/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6892 - auc: 0.5460 - val_loss: 0.6897 - val_auc: 0.5427\n",
      "Epoch 38/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6891 - auc: 0.5468 - val_loss: 0.6896 - val_auc: 0.5429\n",
      "Epoch 39/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6891 - auc: 0.5466 - val_loss: 0.6896 - val_auc: 0.5431\n",
      "Epoch 40/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6891 - auc: 0.5467 - val_loss: 0.6896 - val_auc: 0.5431\n",
      "Epoch 41/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6890 - auc: 0.5473 - val_loss: 0.6896 - val_auc: 0.5428\n",
      "Epoch 42/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6890 - auc: 0.5475 - val_loss: 0.6895 - val_auc: 0.5433\n",
      "Epoch 43/100\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 0.6889 - auc: 0.5478 - val_loss: 0.6895 - val_auc: 0.5435\n",
      "Epoch 44/100\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 0.6889 - auc: 0.5479 - val_loss: 0.6895 - val_auc: 0.5435\n",
      "Epoch 45/100\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.6888 - auc: 0.5484 - val_loss: 0.6896 - val_auc: 0.5432\n",
      "Epoch 46/100\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.6888 - auc: 0.5481 - val_loss: 0.6895 - val_auc: 0.5433\n",
      "Epoch 47/100\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 0.6887 - auc: 0.5487 - val_loss: 0.6896 - val_auc: 0.5431\n",
      "Epoch 48/100\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.6887 - auc: 0.5488 - val_loss: 0.6896 - val_auc: 0.5431\n",
      "Epoch 1/3\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.6905 - auc: 0.5384\n",
      "Epoch 2/3\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.6904 - auc: 0.5389\n",
      "Epoch 3/3\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.6902 - auc: 0.5407\n",
      "train idx, test idx [      0       1       2 ... 1309510 1309511 1309512] [1309513 1309514 1309515 ... 1571412 1571413 1571414]\n",
      "train/test (1309513, 130) (261902, 130) (1309513, 5) (261902, 5)\n",
      "Epoch 1/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.8588 - auc: 0.5060 - val_loss: 0.6954 - val_auc: 0.5307\n",
      "Epoch 2/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.7723 - auc: 0.5094 - val_loss: 0.6914 - val_auc: 0.5365\n",
      "Epoch 3/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.7419 - auc: 0.5114 - val_loss: 0.6906 - val_auc: 0.5397\n",
      "Epoch 4/100\n",
      "320/320 [==============================] - 3s 10ms/step - loss: 0.7244 - auc: 0.5132 - val_loss: 0.6904 - val_auc: 0.5408\n",
      "Epoch 5/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.7129 - auc: 0.5158 - val_loss: 0.6903 - val_auc: 0.5426\n",
      "Epoch 6/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.7049 - auc: 0.5194 - val_loss: 0.6898 - val_auc: 0.5452\n",
      "Epoch 7/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6994 - auc: 0.5238 - val_loss: 0.6897 - val_auc: 0.5454\n",
      "Epoch 8/100\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.6956 - auc: 0.5282 - val_loss: 0.6895 - val_auc: 0.5465\n",
      "Epoch 9/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6933 - auc: 0.5315 - val_loss: 0.6894 - val_auc: 0.5469\n",
      "Epoch 10/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6919 - auc: 0.5340 - val_loss: 0.6893 - val_auc: 0.5475\n",
      "Epoch 11/100\n",
      "320/320 [==============================] - 3s 10ms/step - loss: 0.6911 - auc: 0.5366 - val_loss: 0.6892 - val_auc: 0.5474\n",
      "Epoch 12/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6907 - auc: 0.5382 - val_loss: 0.6892 - val_auc: 0.5477\n",
      "Epoch 13/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6905 - auc: 0.5387 - val_loss: 0.6891 - val_auc: 0.5483\n",
      "Epoch 14/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6904 - auc: 0.5389 - val_loss: 0.6890 - val_auc: 0.5485\n",
      "Epoch 15/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6903 - auc: 0.5400 - val_loss: 0.6889 - val_auc: 0.5487\n",
      "Epoch 16/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6902 - auc: 0.5404 - val_loss: 0.6889 - val_auc: 0.5488\n",
      "Epoch 17/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6901 - auc: 0.5412 - val_loss: 0.6887 - val_auc: 0.5494\n",
      "Epoch 18/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6900 - auc: 0.5413 - val_loss: 0.6887 - val_auc: 0.5499\n",
      "Epoch 19/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6900 - auc: 0.5415 - val_loss: 0.6887 - val_auc: 0.5501\n",
      "Epoch 20/100\n",
      "320/320 [==============================] - 3s 10ms/step - loss: 0.6899 - auc: 0.5422 - val_loss: 0.6885 - val_auc: 0.5506\n",
      "Epoch 21/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6898 - auc: 0.5423 - val_loss: 0.6885 - val_auc: 0.5510\n",
      "Epoch 22/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6898 - auc: 0.5426 - val_loss: 0.6885 - val_auc: 0.5505\n",
      "Epoch 23/100\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6897 - auc: 0.5435 - val_loss: 0.6884 - val_auc: 0.5510\n",
      "Epoch 24/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6897 - auc: 0.5434 - val_loss: 0.6884 - val_auc: 0.5516\n",
      "Epoch 25/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6896 - auc: 0.5438 - val_loss: 0.6884 - val_auc: 0.5514\n",
      "Epoch 26/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6896 - auc: 0.5437 - val_loss: 0.6884 - val_auc: 0.5515\n",
      "Epoch 27/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6895 - auc: 0.5443 - val_loss: 0.6882 - val_auc: 0.5523\n",
      "Epoch 28/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6895 - auc: 0.5445 - val_loss: 0.6882 - val_auc: 0.5524\n",
      "Epoch 29/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6894 - auc: 0.5450 - val_loss: 0.6881 - val_auc: 0.5526\n",
      "Epoch 30/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6893 - auc: 0.5456 - val_loss: 0.6881 - val_auc: 0.5525\n",
      "Epoch 31/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6893 - auc: 0.5455 - val_loss: 0.6881 - val_auc: 0.5526\n",
      "Epoch 32/100\n",
      "320/320 [==============================] - 3s 10ms/step - loss: 0.6893 - auc: 0.5457 - val_loss: 0.6880 - val_auc: 0.5532\n",
      "Epoch 33/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6892 - auc: 0.5458 - val_loss: 0.6879 - val_auc: 0.5535\n",
      "Epoch 34/100\n",
      "320/320 [==============================] - 3s 10ms/step - loss: 0.6892 - auc: 0.5460 - val_loss: 0.6880 - val_auc: 0.5532\n",
      "Epoch 35/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6891 - auc: 0.5463 - val_loss: 0.6880 - val_auc: 0.5535\n",
      "Epoch 36/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6891 - auc: 0.5467 - val_loss: 0.6879 - val_auc: 0.5536\n",
      "Epoch 37/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6890 - auc: 0.5468 - val_loss: 0.6878 - val_auc: 0.5540\n",
      "Epoch 38/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6890 - auc: 0.5470 - val_loss: 0.6878 - val_auc: 0.5542\n",
      "Epoch 39/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6890 - auc: 0.5471 - val_loss: 0.6878 - val_auc: 0.5542\n",
      "Epoch 40/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6890 - auc: 0.5473 - val_loss: 0.6877 - val_auc: 0.5544\n",
      "Epoch 41/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6889 - auc: 0.5478 - val_loss: 0.6877 - val_auc: 0.5545\n",
      "Epoch 42/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6890 - auc: 0.5475 - val_loss: 0.6877 - val_auc: 0.5548\n",
      "Epoch 43/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6889 - auc: 0.5479 - val_loss: 0.6876 - val_auc: 0.5548\n",
      "Epoch 44/100\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.6888 - auc: 0.5481 - val_loss: 0.6877 - val_auc: 0.5547\n",
      "Epoch 45/100\n",
      "320/320 [==============================] - 4s 14ms/step - loss: 0.6888 - auc: 0.5480 - val_loss: 0.6876 - val_auc: 0.5550\n",
      "Epoch 46/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6888 - auc: 0.5483 - val_loss: 0.6876 - val_auc: 0.5552\n",
      "Epoch 47/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6887 - auc: 0.5485 - val_loss: 0.6875 - val_auc: 0.5552\n",
      "Epoch 48/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6888 - auc: 0.5488 - val_loss: 0.6876 - val_auc: 0.5550\n",
      "Epoch 49/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6887 - auc: 0.5490 - val_loss: 0.6876 - val_auc: 0.5549\n",
      "Epoch 50/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6887 - auc: 0.5491 - val_loss: 0.6875 - val_auc: 0.5553\n",
      "Epoch 51/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6887 - auc: 0.5490 - val_loss: 0.6876 - val_auc: 0.5552\n",
      "Epoch 52/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6886 - auc: 0.5493 - val_loss: 0.6875 - val_auc: 0.5554\n",
      "Epoch 53/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6885 - auc: 0.5497 - val_loss: 0.6875 - val_auc: 0.5558\n",
      "Epoch 54/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6886 - auc: 0.5492 - val_loss: 0.6874 - val_auc: 0.5560\n",
      "Epoch 55/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6885 - auc: 0.5501 - val_loss: 0.6875 - val_auc: 0.5556\n",
      "Epoch 56/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6885 - auc: 0.5501 - val_loss: 0.6874 - val_auc: 0.5557\n",
      "Epoch 57/100\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6885 - auc: 0.5502 - val_loss: 0.6874 - val_auc: 0.5559\n",
      "Epoch 58/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6884 - auc: 0.5503 - val_loss: 0.6874 - val_auc: 0.5558\n",
      "Epoch 59/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6884 - auc: 0.5506 - val_loss: 0.6873 - val_auc: 0.5560\n",
      "Epoch 60/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6884 - auc: 0.5504 - val_loss: 0.6874 - val_auc: 0.5560\n",
      "Epoch 61/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6884 - auc: 0.5501 - val_loss: 0.6874 - val_auc: 0.5560\n",
      "Epoch 62/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6884 - auc: 0.5506 - val_loss: 0.6874 - val_auc: 0.5558\n",
      "Epoch 63/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6883 - auc: 0.5508 - val_loss: 0.6874 - val_auc: 0.5561\n",
      "Epoch 64/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6883 - auc: 0.5512 - val_loss: 0.6873 - val_auc: 0.5562\n",
      "Epoch 65/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6882 - auc: 0.5513 - val_loss: 0.6873 - val_auc: 0.5564\n",
      "Epoch 66/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6882 - auc: 0.5511 - val_loss: 0.6873 - val_auc: 0.5565\n",
      "Epoch 67/100\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6882 - auc: 0.5511 - val_loss: 0.6873 - val_auc: 0.5561\n",
      "Epoch 68/100\n",
      "320/320 [==============================] - 3s 10ms/step - loss: 0.6881 - auc: 0.5516 - val_loss: 0.6873 - val_auc: 0.5563\n",
      "Epoch 69/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6881 - auc: 0.5515 - val_loss: 0.6873 - val_auc: 0.5563\n",
      "Epoch 70/100\n",
      "320/320 [==============================] - 3s 10ms/step - loss: 0.6881 - auc: 0.5516 - val_loss: 0.6873 - val_auc: 0.5564\n",
      "Epoch 71/100\n",
      "320/320 [==============================] - 3s 9ms/step - loss: 0.6881 - auc: 0.5520 - val_loss: 0.6872 - val_auc: 0.5564\n",
      "Epoch 1/3\n",
      "64/64 [==============================] - 1s 8ms/step - loss: 0.6884 - auc: 0.5500\n",
      "Epoch 2/3\n",
      "64/64 [==============================] - 1s 9ms/step - loss: 0.6884 - auc: 0.5497\n",
      "Epoch 3/3\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.6882 - auc: 0.5511\n"
     ]
    }
   ],
   "source": [
    "# split data for training and train model\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "max_train_size = None\n",
    "test_size = None\n",
    "gap = 0     # *** ? raise this to prevent info bleed\n",
    "\n",
    "\n",
    "seed = 42\n",
    "fold = 0\n",
    "\n",
    "# starts with 1/5 of data, then 2/5... \n",
    "# note that by splitting the data this way the network will skew towards newer information\n",
    "for train_index, test_index in tscv.split(X):\n",
    "\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    n_inputs = X_train.shape[1]\n",
    "    n_outputs = y_train.shape[1]\n",
    "\n",
    "    print('train idx, test idx', train_index, test_index)\n",
    "    print('train/test', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    model = create_model(n_inputs, n_outputs, encoder)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=4096, \n",
    "              callbacks=[EarlyStopping(monitor='val_auc', mode='max', patience=5, restore_best_weights=True)])\n",
    "    model.save(f'model_{seed}_{fold}.hdf5')\n",
    "\n",
    "    # rebuild and fine tune model\n",
    "    model.compile(Adam(lr=0.00001), loss=BinaryCrossentropy(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    ft_history = model.fit(X_test, y_test, epochs=3, batch_size=4096)\n",
    "    model.save(f'fine_tuned_model{seed}_{fold}.hdf5')\n",
    "\n",
    "\n",
    "    # if training hold out last batch of data to check accuracy\n",
    "    fold += 1\n",
    "    #if TRAINING:\n",
    "    #    if fold >= (n_splits):\n",
    "    #        print('Stopping on fold %d to preserve test set' % fold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:53:37.761998Z",
     "iopub.status.busy": "2020-12-31T16:53:37.758744Z",
     "iopub.status.idle": "2020-12-31T16:53:37.767127Z",
     "shell.execute_reply": "2020-12-31T16:53:37.766348Z"
    },
    "papermill": {
     "duration": 3.276741,
     "end_time": "2020-12-31T16:53:37.767272",
     "exception": false,
     "start_time": "2020-12-31T16:53:34.490531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/model_42_2.hdf5\n",
      "/kaggle/working/fine_tuned_model42_0.hdf5\n",
      "/kaggle/working/model_42_0.hdf5\n",
      "/kaggle/working/model_42_3.hdf5\n",
      "/kaggle/working/__notebook__.ipynb\n",
      "/kaggle/working/fine_tuned_model42_4.hdf5\n",
      "/kaggle/working/fine_tuned_model42_1.hdf5\n",
      "/kaggle/working/model_42_4.hdf5\n",
      "/kaggle/working/fine_tuned_model42_2.hdf5\n",
      "/kaggle/working/saved_model_encoder.hdf5\n",
      "/kaggle/working/fine_tuned_model42_3.hdf5\n",
      "/kaggle/working/model_42_1.hdf5\n",
      "/kaggle/working/encoder.hdf5\n"
     ]
    }
   ],
   "source": [
    "# check saved models\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:53:44.478473Z",
     "iopub.status.busy": "2020-12-31T16:53:44.476627Z",
     "iopub.status.idle": "2020-12-31T16:53:44.479161Z",
     "shell.execute_reply": "2020-12-31T16:53:44.479670Z"
    },
    "papermill": {
     "duration": 3.488807,
     "end_time": "2020-12-31T16:53:44.479794",
     "exception": false,
     "start_time": "2020-12-31T16:53:40.990987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "def check_scores(model, X_tr, X_val, y_tr, y_val):\n",
    "\n",
    "    predict_val = model.predict(X_val)\n",
    "    predict_val = predict_val.mean(axis=1)\n",
    "    predict_val = np.where(predict_val > threshold, 1, 0).astype('int')\n",
    "\n",
    "    \n",
    "    predict_train = model.predict(X_tr)\n",
    "    predict_train = predict_train.mean(axis=1)\n",
    "    predict_train = np.where(predict_train > threshold, 1, 0).astype('int')\n",
    "    \n",
    "    \n",
    "    y_tr = y_tr.mean(axis=1)\n",
    "    y_tr = np.where(y_tr > threshold, 1, 0).astype('int')\n",
    "    \n",
    "    y_val = y_val.mean(axis=1)\n",
    "    y_val = np.where(y_val > threshold, 1, 0).astype('int')\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    score =  roc_auc_score(y_tr, predict_train)\n",
    "    print('\\n\\nTrain score', score)\n",
    "\n",
    "    score = roc_auc_score(y_val, predict_val)\n",
    "    print('Validation score', score)\n",
    "\n",
    "   \n",
    "    \n",
    "    cm = confusion_matrix(y_val, predict_val)\n",
    "    print('\\n\\ntrue n %d, false p %d, false n %d, true p %d' %(cm[0][0], cm[0][1], cm[1][0], cm[1][1]))\n",
    "    print(cm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:53:51.322337Z",
     "iopub.status.busy": "2020-12-31T16:53:51.321783Z",
     "iopub.status.idle": "2020-12-31T16:53:51.325919Z",
     "shell.execute_reply": "2020-12-31T16:53:51.326333Z"
    },
    "papermill": {
     "duration": 3.625339,
     "end_time": "2020-12-31T16:53:51.326443",
     "exception": false,
     "start_time": "2020-12-31T16:53:47.701104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# if training, pull out unseen test data and see if it looks okay ( not all 1s or 0s) and is close to training accuracy\\nif TRAINING:\\n\\n    # split into test/train for sanity checking\\n    n_train = len(train)\\n    n_test = int(n_train * .1)\\n    train_idx = 0\\n    test_idx = n_train - n_test\\n\\n    print(X_train.shape, y_train.shape)\\n    print(X_test.shape, y_test.shape)\\n\\n    n_inputs = X_train.shape[1]\\n    n_outputs = y_train.shape[1]\\n\\n\\n    check_scores(model, X_train, X_test, y_train, y_test)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# if training, pull out unseen test data and see if it looks okay ( not all 1s or 0s) and is close to training accuracy\n",
    "if TRAINING:\n",
    "\n",
    "    # split into test/train for sanity checking\n",
    "    n_train = len(train)\n",
    "    n_test = int(n_train * .1)\n",
    "    train_idx = 0\n",
    "    test_idx = n_train - n_test\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "\n",
    "    n_inputs = X_train.shape[1]\n",
    "    n_outputs = y_train.shape[1]\n",
    "\n",
    "\n",
    "    check_scores(model, X_train, X_test, y_train, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:53:57.976105Z",
     "iopub.status.busy": "2020-12-31T16:53:57.975465Z",
     "iopub.status.idle": "2020-12-31T16:53:57.978409Z",
     "shell.execute_reply": "2020-12-31T16:53:57.978835Z"
    },
    "papermill": {
     "duration": 3.206031,
     "end_time": "2020-12-31T16:53:57.978955",
     "exception": false,
     "start_time": "2020-12-31T16:53:54.772924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nif TRAINING:\\n    \\n    test_df = pd.read_csv('/kaggle/input/jane-street-market-prediction/example_test.csv')\\n    test_df = test_df.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\\n\\n    \\n    \\n    import janestreet\\n    env = janestreet.make_env()\\n    th = 0.5\\n\\n    for (test_df, pred_df) in env.iter_test():\\n\\n        if test_df['weight'].item() > 0:\\n            # fetch a row, convert to numpy array\\n            x_tt = test_df.loc[:, features].values\\n\\n            # if nan use mean\\n            if np.isnan(x_tt[:, 1:].sum()):\\n                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\\n\\n            # use model to make prediction\\n            preds = model(x_tt)\\n\\n            # model targets all 5 returns, get median return prediction\\n            preds = np.median(preds, axis=1)  \\n\\n            # compare predicted return to threshold and buy if over threshold\\n            pred_df.action = np.where(preds >= th, 1, 0).astype(int)\\n        else:\\n            # if weight 0 pass on this one\\n            pred_df.action = 0\\n\\n        env.predict(pred_df)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *** load saved model here? \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "if TRAINING:\n",
    "    \n",
    "    test_df = pd.read_csv('/kaggle/input/jane-street-market-prediction/example_test.csv')\n",
    "    test_df = test_df.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
    "\n",
    "    \n",
    "    \n",
    "    import janestreet\n",
    "    env = janestreet.make_env()\n",
    "    th = 0.5\n",
    "\n",
    "    for (test_df, pred_df) in env.iter_test():\n",
    "\n",
    "        if test_df['weight'].item() > 0:\n",
    "            # fetch a row, convert to numpy array\n",
    "            x_tt = test_df.loc[:, features].values\n",
    "\n",
    "            # if nan use mean\n",
    "            if np.isnan(x_tt[:, 1:].sum()):\n",
    "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "\n",
    "            # use model to make prediction\n",
    "            preds = model(x_tt)\n",
    "\n",
    "            # model targets all 5 returns, get median return prediction\n",
    "            preds = np.median(preds, axis=1)  \n",
    "\n",
    "            # compare predicted return to threshold and buy if over threshold\n",
    "            pred_df.action = np.where(preds >= th, 1, 0).astype(int)\n",
    "        else:\n",
    "            # if weight 0 pass on this one\n",
    "            pred_df.action = 0\n",
    "\n",
    "        env.predict(pred_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T16:54:04.586439Z",
     "iopub.status.busy": "2020-12-31T16:54:04.585903Z",
     "iopub.status.idle": "2020-12-31T16:57:19.255613Z",
     "shell.execute_reply": "2020-12-31T16:57:19.254510Z"
    },
    "papermill": {
     "duration": 198.081889,
     "end_time": "2020-12-31T16:57:19.255739",
     "exception": false,
     "start_time": "2020-12-31T16:54:01.173850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *** load saved model here? \n",
    "# encoder = tf.keras.models.load_model('saved_model_encoder.hdf5')\n",
    "# model = tf.keras.modesl.load(model('fine_tuned_model42_5.hdf5'))\n",
    "\n",
    "Submission = True\n",
    "\n",
    "if Submission:\n",
    "    \n",
    "    \n",
    "    import janestreet\n",
    "    env = janestreet.make_env()\n",
    "    th = 0.5\n",
    "\n",
    "    for (test_df, pred_df) in env.iter_test():\n",
    "\n",
    "        if test_df['weight'].item() > 0:\n",
    "            # fetch a row, convert to numpy array\n",
    "            x_tt = test_df.loc[:, features].values\n",
    "\n",
    "            # if nan use mean\n",
    "            if np.isnan(x_tt[:, 1:].sum()):\n",
    "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "\n",
    "            # use model to make prediction\n",
    "            preds = model(x_tt)\n",
    "\n",
    "            # model targets all 5 returns, get median return prediction\n",
    "            preds = np.median(preds, axis=1)  \n",
    "\n",
    "            # compare predicted return to threshold and buy if over threshold\n",
    "            pred_df.action = np.where(preds >= th, 1, 0).astype(int)\n",
    "        else:\n",
    "            # if weight 0 pass on this one\n",
    "            pred_df.action = 0\n",
    "\n",
    "        env.predict(pred_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.153224,
     "end_time": "2020-12-31T16:57:26.021589",
     "exception": false,
     "start_time": "2020-12-31T16:57:22.868365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "papermill": {
   "duration": 1143.021564,
   "end_time": "2020-12-31T16:57:31.009520",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-31T16:38:27.987956",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
